{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#game classes and helper classes\n",
    "import TTT as T\n",
    "import Mancala as M\n",
    "\n",
    "#for testing\n",
    "import winsound\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "These methods are used for dealing with file writing and reading\n",
    "* readFile takes a file name and creates the Q tables.  this way we can train the Qtables over time and use them later\n",
    "* writeFile takes the Q tables and a file name.  It writes the Q tables to the file.  Its the boyfriend of readFile\n",
    "* collectMetaData takes the Q tables, a file name, and the number of games played.  it appends the information to the file, this way we can collect information about the Qtables training each cycle.  The method writes the number of games played, the number of states in the Q table, the number of states that have not been reinforced, and the ratio of those two numbers expressed as a percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(fileName):\n",
    "        Q1, Q2= {} , {}\n",
    "        data = np.genfromtxt(fileName, dtype=str, delimiter=\"$\", autostrip = True )\n",
    "        for i in range(0,data.shape[0]):\n",
    "            if data[i,0] == \"Q1\":\n",
    "                Q1[ast.literal_eval(data[i,1])] = ast.literal_eval(data[i,2])\n",
    "            if data[i,0] == \"Q2\":\n",
    "                Q2[ast.literal_eval(data[i,1])] = ast.literal_eval(data[i,2])\n",
    "        return Q1,Q2\n",
    "\n",
    "def writeFile(Q1, Q2,fileName):\n",
    "        writer = open(fileName, 'w')\n",
    "        for i in Q1:\n",
    "            writer.write(\"Q1\"+\" $ \" +str(i) +\" $ \"+str(Q1[i])+'\\n')\n",
    "        for i in Q2:\n",
    "            writer.write(\"Q2\"+\" $ \" +str(i) +\" $ \"+str(Q2[i])+'\\n')\n",
    "        writer.close()\n",
    "\n",
    "def collectMetaData(Q1, Q2,fileName,gamesPlayed):\n",
    "        writer = open(fileName, 'a')\n",
    "        count, revisited = 0, 0\n",
    "        for i in Q1:\n",
    "            count += 1\n",
    "            if Q1[i] == -0.5:\n",
    "                revisited +=1\n",
    "        for i in Q2:\n",
    "            count += 1\n",
    "            if Q2[i] == -0.5:\n",
    "                revisited +=1\n",
    "        writer.write(str(gamesPlayed)  +\" $ \" +str(count) +\" $ \"+str(revisited)+\" $ \"+str(round(revisited/(count/100),2))+'\\n')\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "* *trainQ*  takes the number of repetitions, learningRate, epsilonDecayFactor, the game you are playing, and the Q tables.  It plays the games repeatedly each time, lowering the epsilon by multiplying it with the epsilonDecayFactor.  it calls runEarlyPlay once for each player, then runPlay until the game is over, lastly, it calls finalReinforcement.\n",
    "\n",
    "* *runPlay* takes the game, the current players Q table, a copy of the old board, the last move, the epsilon, and the learning rate.  This method is how each player takes there turn.  The method calls getMoves for a list of moves then calls makeMove to make the move.  It checks to see if this state is in the Q table and if the game is over, then the Qtables old board/move state is reinforced by this state.  the BoardOld and moveOld values are assigned and returned along with a boolean.\n",
    "\n",
    "* *runEarlyPlay* this is only called for each player first turn.  it takes the game we are playing, the current players Q table, the epsilon, and the learning rate.  It makes sure that the first state is in the Q table. the BoardOld and moveOld values are assigned and returned along with a boolean.\n",
    "\n",
    "* *finalReinforcement* is only called when the game is over.  this provides reinforcement for both players penultimate states.  it takes both players old boards, old moves and Q tables.  it checks to see which player wins and reinforces the Q table accordingly\n",
    "\n",
    "* *epsilonGreedy* is called on the game and take epsilon and the current players Q table.  It gets a list of valid moves, then it checks to see if a random number is less then epsilon, if the random number is less then the method returns a random move, else it returns the best move based on the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilonGreedy(self, epsilon, Q):\n",
    "        valid = self.getMoves()\n",
    "        if np.random.uniform() < epsilon:\n",
    "            return valid[np.random.choice(len(valid))]\n",
    "        else:\n",
    "            Qs = np.array([Q.get(stateMoveTuple(self.getBoard(), move), -1) for move in valid]) \n",
    "            return valid[ np.argmax(Qs) ]\n",
    "        \n",
    "        \n",
    "\n",
    "def finalReinforcement(boardOld1, moveOld1, Q1, boardOld2, moveOld2, Q2, game):\n",
    "        if game.winner() == 1:\n",
    "            Q1[(stateMoveTuple(boardOld1, moveOld1))] = -1.01\n",
    "            Q2[(stateMoveTuple(boardOld2, moveOld2))] = -10.0\n",
    "        elif game.winner() == 2:\n",
    "            Q1[(stateMoveTuple(boardOld1, moveOld1))] = -10.0\n",
    "            Q2[(stateMoveTuple(boardOld2, moveOld2))] = -1.01\n",
    "        else:\n",
    "            Q1[(stateMoveTuple(boardOld1, moveOld1))] = -5.0\n",
    "            Q2[(stateMoveTuple(boardOld2, moveOld2))] = -5.0\n",
    "        \n",
    "        \n",
    "def stateMoveTuple(board, move):\n",
    "        return (tuple(board),move)\n",
    "    \n",
    "    \n",
    "    \n",
    "def runEarlyPlay(game, Q,  epsilon, learningRate):    \n",
    "    move = epsilonGreedy(game, epsilon, Q)\n",
    "    changePlayer = game.makeMove(move)\n",
    "    \n",
    "    if (stateMoveTuple(game.getBoard(), move)) not in Q:\n",
    "        Q[stateMoveTuple(game.getBoard(), move)] = 0\n",
    "    \n",
    "    boardOld, moveOld = copy.deepcopy(game.getBoard()), copy.deepcopy(move)        \n",
    "    return boardOld, moveOld, changePlayer\n",
    "    \n",
    "\n",
    "    \n",
    "def runPlay(game, Q, boardOld, moveOld, epsilon, learningRate):\n",
    "    \n",
    "    move = epsilonGreedy(game, epsilon, Q)\n",
    "    changePlayer = game.makeMove(move)\n",
    "    \n",
    "    if (stateMoveTuple(game.getBoard(), move)) not in Q:\n",
    "        Q[stateMoveTuple(game.getBoard(), move)] = 0\n",
    "    \n",
    "    if game.isOver():\n",
    "        Q[(stateMoveTuple(game.getBoard(), move))] = -1\n",
    "        \n",
    "    Q[stateMoveTuple(boardOld,moveOld)] += learningRate * (-1 + Q[stateMoveTuple(game.getBoard(),move)] - Q[stateMoveTuple(boardOld,moveOld)])\n",
    "    boardOld, moveOld = copy.deepcopy(game.getBoard()), copy.deepcopy(move)        \n",
    "    return boardOld, moveOld, changePlayer\n",
    "    \n",
    "\n",
    "def trainQ(nRepetitions, learningRate, epsilonDecayFactor, game,  Q1, Q2):\n",
    "    epsilon = 1\n",
    "    #boardOld1, moveOld1, boardOld2, moveOld2 = None, None, None, None\n",
    "    for nGames in range(nRepetitions):          \n",
    "        epsilon *= epsilonDecayFactor\n",
    "        game.reset()\n",
    "        flag  = False\n",
    "        \n",
    "        while not game.isOver():\n",
    "            if flag:\n",
    "                if game.player == 1:\n",
    "                    boardOld1, moveOld1, changePlayer = runPlay(game, Q1, boardOld1, moveOld1, epsilon, learningRate)\n",
    "                else:\n",
    "                    boardOld2, moveOld2, changePlayer  = runPlay(game, Q2, boardOld2, moveOld2, epsilon, learningRate)         \n",
    "            else:\n",
    "                if game.player == 1:\n",
    "                    boardOld1, moveOld1, changePlayer = runEarlyPlay(game, Q1, epsilon, learningRate)\n",
    "                else:\n",
    "                    boardOld2, moveOld2, changePlayer  = runEarlyPlay(game, Q2,epsilon, learningRate)\n",
    "                    flag = True\n",
    "            \n",
    "            if game.isOver():\n",
    "                finalReinforcement(boardOld1, moveOld1, Q1, boardOld2, moveOld2, Q2, game)           \n",
    "            if changePlayer:\n",
    "                game.changePlayer()\n",
    "                \n",
    "    return Q1,Q2\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Random(game,moves,Q1,Q2):\n",
    "    move = random.choice(moves)    \n",
    "    return game.makeMove(move)\n",
    "\n",
    "def callMinMax(game,moves,Q1,Q2):\n",
    "    bestMove = minimax(game, (0,0), Q1, Q2, 4)\n",
    "    if bestMove[1] is not None:\n",
    "        return game.makeMove(bestMove[1])\n",
    "    else:\n",
    "        return game.makeMove(random.choice(moves))\n",
    "\n",
    "\n",
    "def QLookUp(game,moves,Q1,Q2):\n",
    "    val = -99.99\n",
    "    bestMove = None\n",
    "    for move in moves:\n",
    "        temp = copy.deepcopy(game.board)\n",
    "        makemove = game.makeMove(move)\n",
    "        if stateMoveTuple(game.board, move) in Q1:\n",
    "            if Q1[stateMoveTuple(game.board, move)] > val:\n",
    "                bestMove = move\n",
    "                val = Q1[stateMoveTuple(game.board, move)]\n",
    "        game.board = copy.deepcopy(temp)\n",
    "    if bestMove is not None:\n",
    "        return game.makeMove(bestMove)\n",
    "    else:\n",
    "        return game.makeMove(random.choice(moves))\n",
    "\n",
    "\n",
    "\n",
    "def minimax(game, lastMove, QP, QO, depthLeft):\n",
    "    if game.isOver():\n",
    "        return[-1.0]      \n",
    "    \n",
    "    if depthLeft == 0:\n",
    "        return [QO[stateMoveTuple(game.board, lastMove)]]\n",
    "    \n",
    "    bestValue, bestMove = None, None\n",
    "    values = []\n",
    "    \n",
    "    for move in game.getMoves():\n",
    "        \n",
    "        temp = copy.deepcopy(game.board)\n",
    "        changePlayer = game.makeMove(move)                      \n",
    "        \n",
    "        if stateMoveTuple(game.board, move) in QP:\n",
    "            game.changePlayer()\n",
    "            reValue = minimax(game, move, QO,QP ,depthLeft-1)\n",
    "            game.changePlayer()\n",
    "            if reValue[0] is not None:\n",
    "                values.append([reValue[0] , move])\n",
    "        \n",
    "        game.board = copy.deepcopy(temp)\n",
    "    #print(depthLeft, \": \", values)\n",
    "\n",
    "    if len(values) > 0:\n",
    "        if depthLeft % 2 == 1:\n",
    "            bestValue = max(values, key = lambda item: item[0] )[0]\n",
    "            bestMove = max(values, key = lambda item: item[0] )[1]\n",
    "        else:\n",
    "            bestValue = min(values, key = lambda item: item[0] )[0]\n",
    "            bestMove = min(values, key = lambda item: item[0] )[1]\n",
    "        #print(depthLeft, \": \", bestMove,bestValue,'\\n')\n",
    "   \n",
    "    return [bestValue, bestMove]\n",
    "\n",
    "      \n",
    "def runGames(game, player1Move, Q1, player2Move, Q2):\n",
    "    changePlayer = False\n",
    "    game.reset()\n",
    "    while not game.isOver():\n",
    "        moves = game.getMoves()\n",
    "        if game.player == 1:\n",
    "            changePlayer = player1Move(game,moves,Q1,Q2)\n",
    "        else:\n",
    "            changePlayer = player2Move(game,moves,Q2,Q1)\n",
    "        if changePlayer:\n",
    "            game.changePlayer()\n",
    "    return game.winner()\n",
    "\n",
    "\n",
    "def tournament(game, strategy1, Q1, strategy2, Q2, rounds):\n",
    "    P1, P2, Tie = 0,0,0\n",
    "    start = time.time()\n",
    "    for i in range(rounds):\n",
    "        winner = runGames(game, strategy1, Q1, strategy2, Q2)\n",
    "        if winner == 1:\n",
    "            P1 += 1\n",
    "        elif winner == 2:\n",
    "            P2 += 1\n",
    "        else:\n",
    "            Tie += 1\n",
    "    print(\"P1 \", strategy1,\" vs P2 \",strategy2)\n",
    "    print(\"player 1 wins \",round(P1/(P1+P2+Tie)*100,2) ,\"% of the time\")\n",
    "    print(\"player 2 wins \",round(P2/(P1+P2+Tie)*100,2) ,\"% of the time\")\n",
    "    print(\"the game Ties \",round(Tie/(P1+P2+Tie)*100,2) ,\"% of the time\")\n",
    "    print(\"Time to play \",rounds,\" games: \",time.time()- start )\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is where the training happens.  you will need to make the CSV files and save them in the same folder as this notebook.  the first time you train the tables you will use empty dictionaries.  All other times you will read in the Q tables with readFile.  the plus zero in collectMetaData is there because we are appending to that file.  the 2nd time you run the for loop the 0 should be replaced with the number of games your Q table has played so far.  The computer will beep when it is done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game = M.Mancala()\n",
    "Q1, Q2 = readFile('M Qtable.csv')\n",
    "i = 0\n",
    "while((i * 100000 + 70000000)< 90000000 ):\n",
    "    R1, R2 = {}, {}\n",
    "    i += 1\n",
    "    trainQ(50000, .5, .999, game,  Q1, Q2)\n",
    "    writeFile(Q1, Q2,'M Qtable.csv')\n",
    "    trainQ(25000, .5, .999, game,  R1, Q2)\n",
    "    writeFile(Q1, Q2,'M Qtable.csv')\n",
    "    trainQ(25000, .5, .999, game,  Q1, R2)\n",
    "    writeFile(Q1, Q2,'M Qtable.csv')\n",
    "    collectMetaData(Q1, Q2, 'M rawData.csv', i * 100000 + 70000000)\n",
    "winsound.Beep(2750,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1  <function Random at 0x000001E0EF80BE18>  vs P2  <function Random at 0x000001E0EF80BE18>\n",
      "player 1 wins  58.5 % of the time\n",
      "player 2 wins  31.0 % of the time\n",
      "the game Ties  10.5 % of the time\n",
      "Time to play  200  games:  0.14063596725463867\n",
      "\n",
      "P1  <function Random at 0x000001E0EF80BE18>  vs P2  <function QLookUp at 0x000001E0EF80BF28>\n",
      "player 1 wins  27.0 % of the time\n",
      "player 2 wins  61.5 % of the time\n",
      "the game Ties  11.5 % of the time\n",
      "Time to play  200  games:  0.5160458087921143\n",
      "\n",
      "P1  <function Random at 0x000001E0EF80BE18>  vs P2  <function callMinMax at 0x000001E0EF80BEA0>\n",
      "player 1 wins  48.0 % of the time\n",
      "player 2 wins  34.5 % of the time\n",
      "the game Ties  17.5 % of the time\n",
      "Time to play  200  games:  88.95846152305603\n",
      "\n",
      "P1  <function QLookUp at 0x000001E0EF80BF28>  vs P2  <function Random at 0x000001E0EF80BE18>\n",
      "player 1 wins  89.5 % of the time\n",
      "player 2 wins  4.0 % of the time\n",
      "the game Ties  6.5 % of the time\n",
      "Time to play  200  games:  0.456768274307251\n",
      "\n",
      "P1  <function QLookUp at 0x000001E0EF80BF28>  vs P2  <function QLookUp at 0x000001E0EF80BF28>\n",
      "player 1 wins  100.0 % of the time\n",
      "player 2 wins  0.0 % of the time\n",
      "the game Ties  0.0 % of the time\n",
      "Time to play  200  games:  0.6583800315856934\n",
      "\n",
      "P1  <function QLookUp at 0x000001E0EF80BF28>  vs P2  <function callMinMax at 0x000001E0EF80BEA0>\n",
      "player 1 wins  100.0 % of the time\n",
      "player 2 wins  0.0 % of the time\n",
      "the game Ties  0.0 % of the time\n",
      "Time to play  200  games:  77.98100781440735\n",
      "\n",
      "P1  <function callMinMax at 0x000001E0EF80BEA0>  vs P2  <function Random at 0x000001E0EF80BE18>\n",
      "player 1 wins  71.0 % of the time\n",
      "player 2 wins  17.0 % of the time\n",
      "the game Ties  12.0 % of the time\n",
      "Time to play  200  games:  142.0824499130249\n",
      "\n",
      "P1  <function callMinMax at 0x000001E0EF80BEA0>  vs P2  <function QLookUp at 0x000001E0EF80BF28>\n",
      "player 1 wins "
     ]
    }
   ],
   "source": [
    "game = T.TTT()\n",
    "Q1, Q2 = readFile('T Qtable.csv')\n",
    "\n",
    "tournament(game, Random, Q1, Random, Q2, 200)\n",
    "tournament(game, Random, Q1, QLookUp, Q2, 200)\n",
    "tournament(game, Random, Q1, callMinMax, Q2, 200)\n",
    "\n",
    "tournament(game, QLookUp, Q1, Random, Q2, 200)\n",
    "tournament(game, QLookUp, Q1, QLookUp, Q2, 200)\n",
    "tournament(game, QLookUp, Q1, callMinMax, Q2, 200)\n",
    "tournament(game, callMinMax, Q1, Random, Q2, 200)\n",
    "tournament(game, callMinMax, Q1, QLookUp, Q2, 200)\n",
    "tournament(game, callMinMax, Q1, callMinMax, Q2, 200)\n",
    "winsound.Beep(2750,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"new 4.csv\", dtype=str, delimiter=\"$\", autostrip = True )\n",
    "for i in range(data.shape[0]):\n",
    "    data[i,0] = ast.literal_eval(data[i,0])+ 20000000\n",
    "writer = open(\"new 4.csv\", 'w')\n",
    "for i in range(data.shape[0]):\n",
    "    writer.write(str(data[i,0])  +\" $ \" +data[i,1] +\" $ \"+data[i,2]+\" $ \"+data[i,3]+'\\n')\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
